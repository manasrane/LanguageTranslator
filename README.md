ğŸŒ Language Translator â€” Seq2Seq Transformer (PyTorch)
A language translation model built from scratch using a custom implementation of the Transformer architecture in PyTorch. This project demonstrates how sequence-to-sequence learning works for neural machine translation (NMT), without using high-level frameworks like Hugging Face.

ğŸš€ Features
Full encoder-decoder Transformer architecture

tokenizer integration from Hugging face

Manual implementation of:

Positional encoding

Scaled dot-product attention

Multi-head attention

Causal masking

SOS/EOS/PAD token handling

Greedy decoding for inference

Training loop, validation, and checkpointing

Dataset preprocessing and batching

Built with torch, datasets, and tokenizers
